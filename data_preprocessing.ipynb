{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "data_preprocessing.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyM9vqkrQvNKlTvluA09TObU",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kathdevx/emotionalAI/blob/main/data_preprocessing.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hf9NaiZ3LTeG"
      },
      "source": [
        "#@title Separation of training files from the dataset\n",
        "\n",
        "with open('/content/drive/Shareddrives/DNN -  audio files/Podcasts_Dataset/Data/train_files.txt', 'w') as outfile:\n",
        "    with open('/content/drive/Shareddrives/DNN -  audio files/Podcasts_Dataset/Data/meta-info/Partitions.txt', 'r') as part_file:\n",
        "        for line in part_file:\n",
        "            idx = line.find(';')\n",
        "            if line[:idx] == 'Train':\n",
        "                outfile.write(line[idx+2:-1])\n",
        "                outfile.write('\\n')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EXuWxMPSMUb7"
      },
      "source": [
        "train_fnames = []\n",
        "\n",
        "with open('/content/drive/Shareddrives/DNN -  audio files/Podcasts_Dataset/Data/train_files.txt', 'r') as infile:\n",
        "    train_fnames = infile.read().splitlines()"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7GqnKxkjGMU4"
      },
      "source": [
        "Kapoia arxeia den vrethikan"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zBZKPFAGhB2G",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "62186fbb-937d-4f6a-8a8d-50eb3093669b"
      },
      "source": [
        "def file_len(fname):\n",
        "    with open(fname) as f:\n",
        "        for i, l in enumerate(f):\n",
        "            pass\n",
        "    return i + 1\n",
        "\n",
        "print(file_len('/content/drive/Shareddrives/DNN -  audio files/Podcasts_Dataset/Data/train_files_path.txt'))"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "43588\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XpIpw6vYxZVo"
      },
      "source": [
        "dir = '/content/drive/Shareddrives/DNN -  audio files/Podcasts_Dataset/Data/Audio Files/'\n",
        "\n",
        "import soundfile as sf\n",
        "import glob\n",
        "\n",
        "train_data = open('/content/drive/Shareddrives/DNN -  audio files/Podcasts_Dataset/Data/train_files_path.txt', 'w')\n",
        "\n",
        "for filename in glob.glob(dir+'*.wav'):\n",
        "    idx = filename.rfind('/')\n",
        "    fname = filename[idx+1:]\n",
        "\n",
        "    if fname in train_fnames:\n",
        "        train_data.write(fname)\n",
        "        train_data.write(' ')\n",
        "        train_data.write(filename)\n",
        "        train_data.write('\\n')\n",
        "\n",
        "train_data.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bn4Ff-aUTUgR"
      },
      "source": [
        "# Begin **HERE**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QhpZciivQLzC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "50fe87d9-2cb2-4374-ffa9-eb5f8e65386d"
      },
      "source": [
        "!pip install soundfile\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: soundfile in /usr/local/lib/python3.7/dist-packages (0.10.3.post1)\n",
            "Requirement already satisfied: cffi>=1.0 in /usr/local/lib/python3.7/dist-packages (from soundfile) (1.14.6)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.7/dist-packages (from cffi>=1.0->soundfile) (2.20)\n",
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "luXP3zbPAi6J",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3cadaec7-e65f-4748-dc80-4769e17bad80"
      },
      "source": [
        "!pip install git+https://github.com/huggingface/transformers"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting git+https://github.com/huggingface/transformers\n",
            "  Cloning https://github.com/huggingface/transformers to /tmp/pip-req-build-nmlnhthr\n",
            "  Running command git clone -q https://github.com/huggingface/transformers /tmp/pip-req-build-nmlnhthr\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "    Preparing wheel metadata ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers==4.10.0.dev0) (21.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers==4.10.0.dev0) (4.41.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers==4.10.0.dev0) (2019.12.20)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers==4.10.0.dev0) (1.19.5)\n",
            "Requirement already satisfied: huggingface-hub==0.0.12 in /usr/local/lib/python3.7/dist-packages (from transformers==4.10.0.dev0) (0.0.12)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from transformers==4.10.0.dev0) (5.4.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers==4.10.0.dev0) (3.0.12)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers==4.10.0.dev0) (4.6.1)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.7/dist-packages (from transformers==4.10.0.dev0) (0.0.45)\n",
            "Requirement already satisfied: tokenizers<0.11,>=0.10.1 in /usr/local/lib/python3.7/dist-packages (from transformers==4.10.0.dev0) (0.10.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers==4.10.0.dev0) (2.23.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from huggingface-hub==0.0.12->transformers==4.10.0.dev0) (3.7.4.3)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->transformers==4.10.0.dev0) (2.4.7)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers==4.10.0.dev0) (3.5.0)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.10.0.dev0) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.10.0.dev0) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.10.0.dev0) (2021.5.30)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.10.0.dev0) (2.10)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==4.10.0.dev0) (1.15.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==4.10.0.dev0) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==4.10.0.dev0) (1.0.1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aRmXN7ZnDl0J",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "720a39c5-507a-489a-8433-982f7e3a2161"
      },
      "source": [
        "import nltk\n",
        "nltk.download('punkt')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8gGxyCnn3dis"
      },
      "source": [
        "import librosa\n",
        "import soundfile as sf\n",
        "import torch\n",
        "from transformers import Wav2Vec2ForCTC, Wav2Vec2Tokenizer\n",
        "import nltk\n",
        "\n",
        "def load_model():\n",
        "    tokenizer = Wav2Vec2Tokenizer.from_pretrained('facebook/wav2vec2-large-960h')\n",
        "    model = Wav2Vec2ForCTC.from_pretrained('facebook/wav2vec2-large-960h')\n",
        "    return tokenizer, model\n",
        "\n",
        "def correct_sentence(input_text):\n",
        "    sentences = nltk.sent_tokenize(input_text)\n",
        "    return (' '.join([s.replace(s[0],s[0].capitalize(),1) for s in sentences]))\n",
        "\n",
        "def asr_transcript(input_file):\n",
        "\n",
        "    tokenizer, model = load_model();\n",
        "    \n",
        "    speech, fs = sf.read(input_file)\n",
        "\n",
        "    if len(speech.shape) > 1: \n",
        "        speech = speech[:,0] + speech[:,1]\n",
        "\n",
        "    if fs != 16000:\n",
        "        speech = librosa.resample(speech, fs, 16000)\n",
        "\n",
        "    input_values = tokenizer(speech, return_tensors=\"pt\").input_values;\n",
        "    logits = model(input_values).logits;\n",
        "    \n",
        "    predicted_ids = torch.argmax(logits, dim=-1)\n",
        "    \n",
        "    transcription = tokenizer.decode(predicted_ids[0]);\n",
        "\n",
        "    return correct_sentence(transcription.lower())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NAXmfDiE9NGR"
      },
      "source": [
        "def chunks(lst, n):\n",
        "    chunky = []\n",
        "    for i in range(0, len(lst), n):\n",
        "        chunky.append(lst[i:i + n])\n",
        "    return chunky"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DcIfh-Fg5KJg"
      },
      "source": [
        "batch_size = 50\n",
        "train_data = []\n",
        "\n",
        "with open('/content/drive/Shareddrives/DNN -  audio files/Podcasts_Dataset/Data/train_files_path.txt', 'r') as tdata:\n",
        "    data = tdata.read().splitlines()\n",
        "    train_data = chunks(data, batch_size)"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q14_6nyvR_em"
      },
      "source": [
        "# Don't uncomment, files already created"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "__pvjMlHGyUJ"
      },
      "source": [
        "# import pickle \n",
        "\n",
        "# for i in range(len(train_data)):\n",
        "#     fname = '/content/drive/Shareddrives/DNN -  audio files/Podcasts_Dataset/Data/training chunks/train_chunk_' + str(i) + '.pkl'\n",
        "#     with open(fname, 'wb') as outfile:\n",
        "#         pickle.dump(train_data[i],outfile)"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yr8o_k5_Sgmk"
      },
      "source": [
        "katerina = range(0, 436)\n",
        "xristina = range(436, 873)\n",
        "global_counter = xristina[0]"
      ],
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x38QzNCWIBKe"
      },
      "source": [
        "while global_counter <= xristina[-1]:\n",
        "    print(f'----------- Epoch {global_counter} ----------- \\n')\n",
        "    inname = '/content/drive/Shareddrives/DNN -  audio files/Podcasts_Dataset/Data/training chunks/train_chunk_' + str(global_counter) + '.pkl'\n",
        "    outname = '/content/drive/Shareddrives/DNN -  audio files/Podcasts_Dataset/Data/training sentences/train_sentences_chunk' + str(global_counter) + '.txt'\n",
        "\n",
        "\n",
        "    train_chunk = None\n",
        "    with open(inname, 'rb') as infile:\n",
        "        train_chunk = pickle.load(infile)\n",
        "    with open(outname, 'w') as outfile:\n",
        "        for t in train_chunk: \n",
        "            temp = t.split(' ', 1)\n",
        "            fname = temp[0] + ' '\n",
        "            outfile.write(fname)\n",
        "            outfile.write(asr_transcript(temp[1]))\n",
        "            outfile.write('\\n')\n",
        "    global_counter +=1\n",
        "    break"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lSuGDfC68z1F"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}